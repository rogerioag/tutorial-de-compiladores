{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Léxica\n",
    "\n",
    "## Teste\n",
    "\n",
    "sdfasdfasdf\n",
    "asdfasdf\n",
    "asdfas\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import argv, exit\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "     level = logging.DEBUG,\n",
    "     filename = \"log.txt\",\n",
    "     filemode = \"w\",\n",
    "     format = \"%(filename)10s:%(lineno)4d:%(message)s\"\n",
    ")\n",
    "log = logging.getLogger()\n",
    "\n",
    "import ply.lex as lex\n",
    "from ply.lex import TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    \"ID\",  # identificador\n",
    "    # numerais\n",
    "    \"NUM_NOTACAO_CIENTIFICA\",  # ponto flutuante em notaçao científica\n",
    "    \"NUM_PONTO_FLUTUANTE\",  # ponto flutuate\n",
    "    \"NUM_INTEIRO\",  # inteiro\n",
    "    # operadores binarios\n",
    "    \"ADICAO\",  # +\n",
    "    \"SUBTRACAO\",  # -\n",
    "    \"MULTIPLICACAO\",  # *\n",
    "    \"DIVISAO\",  # /\n",
    "    \"E_LOGICO\",  # &&\n",
    "    \"OU_LOGICO\",  # ||\n",
    "    \"DIFERENCA\",  # <>\n",
    "    \"MENOR_IGUAL\",  # <=\n",
    "    \"MAIOR_IGUAL\",  # >=\n",
    "    \"MENOR\",  # <\n",
    "    \"MAIOR\",  # >\n",
    "    \"IGUALDADE\",  # =\n",
    "    # operadores unarios\n",
    "    \"NEGACAO\",  # !\n",
    "    # simbolos\n",
    "    \"ABRE_PAR\",  # (\n",
    "    \"FECHA_PAR\",  # )\n",
    "    \"ABRE_COL\",  # [\n",
    "    \"FECHA_COL\",  # ]\n",
    "    \"VIRGULA\",  # ,\n",
    "    \"DOIS_PONTOS\",  # :\n",
    "    \"ATRIBUICAO\",  # :=\n",
    "    # 'COMENTARIO', # {***}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_words = {\n",
    "    \"se\": \"SE\",\n",
    "    \"então\": \"ENTAO\",\n",
    "    \"senão\": \"SENAO\",\n",
    "    \"fim\": \"FIM\",\n",
    "    \"repita\": \"REPITA\",\n",
    "    \"flutuante\": \"FLUTUANTE\",\n",
    "    \"retorna\": \"RETORNA\",\n",
    "    \"até\": \"ATE\",\n",
    "    \"leia\": \"LEIA\",\n",
    "    \"escreva\": \"ESCREVA\",\n",
    "    \"inteiro\": \"INTEIRO\",\n",
    "}\n",
    "\n",
    "tokens = tokens + list(reserved_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "digito = r\"([0-9])\"\n",
    "letra = r\"([a-zA-ZáÁãÃàÀéÉíÍóÓõÕ])\"\n",
    "sinal = r\"([\\-\\+]?)\"\n",
    "\n",
    "\"\"\" \n",
    "    id deve começar com uma letra\n",
    "\"\"\"\n",
    "id = (\n",
    "    r\"(\" + letra + r\"(\" + digito + r\"+|_|\" + letra + r\")*)\"\n",
    ")  # o mesmo que '((letra)(letra|_|([0-9]))*)'\n",
    "\n",
    "inteiro = r\"(\" + sinal + digito + r\"+)\"\n",
    "\n",
    "flutuante = (\n",
    "    # r\"(\" + digito + r\"+\\.\" + digito + r\"+?)\"\n",
    "    # (([-\\+]?)([0-9]+)\\.([0-9]+))'\n",
    "    r'\\d+[eE][-+]?\\d+|(\\.\\d+|\\d+\\.\\d*)([eE][-+]?\\d+)?'\n",
    "    # r'[-+]?[0-9]+(\\.([0-9]+)?)'\n",
    "    #r'[+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?'\n",
    "    #r\"(([-\\+]?)([0-9]+)\\.([0-9]+))\"\n",
    "    )\n",
    "\n",
    "notacao_cientifica = (\n",
    "    r\"(\" + sinal + r\"([1-9])\\.\" + digito + r\"+[eE]\" + sinal + digito + r\"+)\"\n",
    ")  # o mesmo que '(([-\\+]?)([1-9])\\.([0-9])+[eE]([-\\+]?)([0-9]+))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Expressões Regulaes para tokens simples.\n",
    "# Símbolos.\n",
    "t_ADICAO    = r'\\+'\n",
    "t_SUBTRACAO  = r'-'\n",
    "t_MULTIPLICACAO   = r'\\*'\n",
    "t_DIVISAO = r'/'\n",
    "t_ABRE_PAR  = r'\\('\n",
    "t_FECHA_PAR  = r'\\)'\n",
    "t_ABRE_COL = r'\\['\n",
    "t_FECHA_COL = r'\\]'\n",
    "t_VIRGULA = r','\n",
    "t_ATRIBUICAO = r':='\n",
    "t_DOIS_PONTOS = r':'\n",
    "\n",
    "# Operadores Lógicos.\n",
    "t_E_LOGICO = r'&&'\n",
    "t_OU_LOGICO = r'\\|\\|'\n",
    "t_NEGACAO = r'!'\n",
    "\n",
    "# Operadores Relacionais.\n",
    "t_DIFERENCA = r'<>'\n",
    "t_MENOR_IGUAL = r'<='\n",
    "t_MAIOR_IGUAL = r'>='\n",
    "t_MENOR = r'<'\n",
    "t_MAIOR = r'>'\n",
    "t_IGUALDADE = r'='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TOKEN(id)\n",
    "def t_ID(token):\n",
    "    token.type = reserved_words.get(\n",
    "        token.value, \"ID\"\n",
    "    )  # não é necessário fazer regras/regex para cada palavra reservada\n",
    "    # se o token não for uma palavra reservada automaticamente é um id\n",
    "    # As palavras reservadas têm precedências sobre os ids\n",
    "\n",
    "    return token\n",
    "\n",
    "@TOKEN(notacao_cientifica)\n",
    "def t_NUM_NOTACAO_CIENTIFICA(token):\n",
    "    return token\n",
    "\n",
    "@TOKEN(flutuante)\n",
    "def t_NUM_PONTO_FLUTUANTE(token):\n",
    "    return token\n",
    "\n",
    "@TOKEN(inteiro)\n",
    "def t_NUM_INTEIRO(token):\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ignore = \" \\t\"\n",
    "\n",
    "# t_COMENTARIO = r'(\\{((.|\\n)*?)\\})'\n",
    "# para poder contar as quebras de linha dentro dos comentarios\n",
    "def t_COMENTARIO(token):\n",
    "    r\"(\\{((.|\\n)*?)\\})\"\n",
    "    token.lexer.lineno += token.value.count(\"\\n\")\n",
    "    # return token\n",
    "\n",
    "def t_newline(token):\n",
    "    r\"\\n+\"\n",
    "    token.lexer.lineno += len(token.value)\n",
    "\n",
    "def define_column(input, lexpos):\n",
    "    begin_line = input.rfind(\"\\n\", 0, lexpos) + 1\n",
    "    return (lexpos - begin_line) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_error(token):\n",
    "\n",
    "    # file = token.lexer.filename\n",
    "    line = token.lineno\n",
    "    # column = define_column(token.lexer.backup_data, token.lexpos)\n",
    "    message = \"Caracter ilegal '%s'\" % token.value[0]\n",
    "\n",
    "    # print(f\"[{file}]:[{line},{column}]: {message}.\") \n",
    "    print(message)\n",
    "\n",
    "    token.lexer.skip(1)\n",
    "\n",
    "    # token.lexer.has_error = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    argv[1] = 'teste.tpp'\n",
    "    aux = argv[1].split('.')\n",
    "    if aux[-1] != 'tpp':\n",
    "      raise IOError(\"Not a .tpp file!\")\n",
    "    data = open(argv[1])\n",
    "\n",
    "    source_file = data.read()\n",
    "    lexer.input(source_file)\n",
    "\n",
    "    # Tokenize\n",
    "    while True:\n",
    "      tok = lexer.token()\n",
    "      if not tok: \n",
    "        break      # No more input\n",
    "      print(tok)\n",
    "      # print(tok.type)\n",
    "      #print(tok.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(INTEIRO,'inteiro',1,0)\n",
      "LexToken(DOIS_PONTOS,':',1,7)\n",
      "LexToken(ID,'a',1,9)\n",
      "LexToken(ABRE_COL,'[',1,10)\n",
      "LexToken(NUM_INTEIRO,'10',1,11)\n",
      "LexToken(FECHA_COL,']',1,13)\n",
      "LexToken(FLUTUANTE,'flutuante',2,15)\n",
      "LexToken(DOIS_PONTOS,':',2,24)\n",
      "LexToken(ID,'b',2,26)\n",
      "LexToken(INTEIRO,'inteiro',4,29)\n",
      "LexToken(ID,'func1',4,37)\n",
      "LexToken(ABRE_PAR,'(',4,42)\n",
      "LexToken(INTEIRO,'inteiro',4,43)\n",
      "LexToken(DOIS_PONTOS,':',4,50)\n",
      "LexToken(ID,'x',4,51)\n",
      "LexToken(VIRGULA,',',4,52)\n",
      "LexToken(FLUTUANTE,'flutuante',4,54)\n",
      "LexToken(DOIS_PONTOS,':',4,63)\n",
      "LexToken(ID,'y',4,64)\n",
      "LexToken(FECHA_PAR,')',4,65)\n",
      "LexToken(INTEIRO,'inteiro',5,69)\n",
      "LexToken(DOIS_PONTOS,':',5,76)\n",
      "LexToken(ID,'res',5,78)\n",
      "LexToken(SE,'se',6,84)\n",
      "LexToken(ABRE_PAR,'(',6,87)\n",
      "LexToken(ID,'x',6,88)\n",
      "LexToken(MAIOR,'>',6,90)\n",
      "LexToken(ID,'y',6,92)\n",
      "LexToken(FECHA_PAR,')',6,93)\n",
      "LexToken(ENTAO,'então',6,95)\n",
      "LexToken(ID,'res',7,105)\n",
      "LexToken(ATRIBUICAO,':=',7,109)\n",
      "LexToken(ID,'x',7,112)\n",
      "LexToken(ADICAO,'+',7,114)\n",
      "LexToken(ID,'y',7,116)\n",
      "LexToken(SENAO,'senão',8,120)\n",
      "LexToken(ID,'res',9,130)\n",
      "LexToken(ATRIBUICAO,':=',9,134)\n",
      "LexToken(ID,'x',9,137)\n",
      "LexToken(MULTIPLICACAO,'*',9,139)\n",
      "LexToken(ID,'y',9,141)\n",
      "LexToken(FIM,'fim',10,145)\n",
      "LexToken(RETORNA,'retorna',11,151)\n",
      "LexToken(ABRE_PAR,'(',11,158)\n",
      "LexToken(ID,'res',11,159)\n",
      "LexToken(FECHA_PAR,')',11,162)\n",
      "LexToken(FIM,'fim',12,164)\n",
      "LexToken(ID,'func2',14,169)\n",
      "LexToken(ABRE_PAR,'(',14,174)\n",
      "LexToken(INTEIRO,'inteiro',14,175)\n",
      "LexToken(DOIS_PONTOS,':',14,182)\n",
      "LexToken(ID,'z',14,183)\n",
      "LexToken(VIRGULA,',',14,184)\n",
      "LexToken(FLUTUANTE,'flutuante',14,186)\n",
      "LexToken(DOIS_PONTOS,':',14,195)\n",
      "LexToken(ID,'w',14,196)\n",
      "LexToken(FECHA_PAR,')',14,197)\n",
      "LexToken(ID,'a',15,201)\n",
      "LexToken(ATRIBUICAO,':=',15,203)\n",
      "LexToken(ID,'z',15,206)\n",
      "LexToken(ID,'b',16,210)\n",
      "LexToken(ATRIBUICAO,':=',16,212)\n",
      "LexToken(ID,'w',16,215)\n",
      "LexToken(FIM,'fim',17,217)\n",
      "LexToken(INTEIRO,'inteiro',19,222)\n",
      "LexToken(ID,'principal',19,230)\n",
      "LexToken(ABRE_PAR,'(',19,239)\n",
      "LexToken(FECHA_PAR,')',19,240)\n",
      "LexToken(INTEIRO,'inteiro',20,244)\n",
      "LexToken(DOIS_PONTOS,':',20,251)\n",
      "LexToken(ID,'x',20,253)\n",
      "LexToken(VIRGULA,',',20,254)\n",
      "LexToken(ID,'y',20,255)\n",
      "LexToken(FLUTUANTE,'flutuante',21,259)\n",
      "LexToken(DOIS_PONTOS,':',21,268)\n",
      "LexToken(ID,'w',21,270)\n",
      "LexToken(ID,'a',22,274)\n",
      "LexToken(ATRIBUICAO,':=',22,276)\n",
      "LexToken(NUM_INTEIRO,'10',22,279)\n",
      "LexToken(ADICAO,'+',22,282)\n",
      "LexToken(NUM_INTEIRO,'2',22,284)\n",
      "LexToken(LEIA,'leia',23,288)\n",
      "LexToken(ABRE_PAR,'(',23,292)\n",
      "LexToken(ID,'x',23,293)\n",
      "LexToken(FECHA_PAR,')',23,294)\n",
      "LexToken(LEIA,'leia',24,298)\n",
      "LexToken(ABRE_PAR,'(',24,302)\n",
      "LexToken(ID,'w',24,303)\n",
      "LexToken(FECHA_PAR,')',24,304)\n",
      "LexToken(ID,'w',25,308)\n",
      "LexToken(ATRIBUICAO,':=',25,310)\n",
      "LexToken(NUM_PONTO_FLUTUANTE,'.6',25,313)\n",
      "LexToken(ADICAO,'+',25,316)\n",
      "LexToken(NUM_PONTO_FLUTUANTE,'1.',25,318)\n",
      "LexToken(ID,'func2',26,323)\n",
      "LexToken(ABRE_PAR,'(',26,328)\n",
      "LexToken(NUM_INTEIRO,'1',26,329)\n",
      "LexToken(VIRGULA,',',26,330)\n",
      "LexToken(NUM_PONTO_FLUTUANTE,'2.5',26,332)\n",
      "LexToken(FECHA_PAR,')',26,335)\n",
      "LexToken(ID,'b',27,339)\n",
      "LexToken(ATRIBUICAO,':=',27,341)\n",
      "LexToken(ID,'func1',27,344)\n",
      "LexToken(ABRE_PAR,'(',27,349)\n",
      "LexToken(ID,'x',27,350)\n",
      "LexToken(VIRGULA,',',27,351)\n",
      "LexToken(ID,'w',27,352)\n",
      "LexToken(FECHA_PAR,')',27,353)\n",
      "LexToken(ESCREVA,'escreva',28,357)\n",
      "LexToken(ABRE_PAR,'(',28,364)\n",
      "LexToken(ID,'b',28,365)\n",
      "LexToken(FECHA_PAR,')',28,366)\n",
      "LexToken(RETORNA,'retorna',29,370)\n",
      "LexToken(ABRE_PAR,'(',29,377)\n",
      "LexToken(NUM_INTEIRO,'0',29,378)\n",
      "LexToken(FECHA_PAR,')',29,379)\n",
      "LexToken(FIM,'fim',30,381)\n"
     ]
    }
   ],
   "source": [
    "# Build the lexer.\n",
    "__file__ = \"01-compiladores-analise-lexica-tpplex.ipynb\"\n",
    "lexer = lex.lex(optimize=True,debug=True,debuglog=log)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
